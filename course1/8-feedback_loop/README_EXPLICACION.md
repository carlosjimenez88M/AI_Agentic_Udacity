# ğŸ”„ Feedback Loops con LLMs - GuÃ­a Completa

## ğŸ“š Â¿QuÃ© es un Feedback Loop?

Un **Feedback Loop** (ciclo de retroalimentaciÃ³n) es un patrÃ³n donde:
1. El LLM genera algo (cÃ³digo, texto, etc.)
2. Evaluamos el resultado automÃ¡ticamente (tests, validaciones)
3. Si no es correcto, le damos feedback especÃ­fico al LLM
4. El LLM mejora su respuesta basÃ¡ndose en ese feedback
5. Repetimos hasta lograr el resultado deseado

---

## ğŸ¯ Objetivo del Ejercicio

Crear una funciÃ³n `process_data()` que:
- Calcule **suma**, **promedio** o **mediana** de una lista
- Filtre valores no numÃ©ricos (strings, None, etc.)
- Maneje listas vacÃ­as retornando `None`
- Valide modos invÃ¡lidos levantando `ValueError`

**Sin escribir el cÃ³digo nosotros mismos** - el LLM lo genera y mejora iterativamente.

---

## ğŸ—ï¸ Arquitectura del Feedback Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FEEDBACK LOOP CYCLE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DEFINIR TAREA          2. GENERAR CÃ“DIGO
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Task     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚   LLM    â”‚
   â”‚ + Tests  â”‚              â”‚ Generate â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
5. MEJORAR               3. EJECUTAR & PROBAR
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   LLM    â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Run     â”‚
   â”‚ Improve  â”‚              â”‚  Tests   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                           â”‚
       â”‚                           â–¼
       â”‚                    4. GENERAR FEEDBACK
       â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Format   â”‚
           (si fallan tests)   â”‚ Feedback â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Paso a Paso - ImplementaciÃ³n Completa

### PASO 1: Definir la Tarea Claramente

```python
task_description = """
Create a Python function called `process_data` that:

1. Accept a list as first parameter and 'mode' parameter (default: 'average')
2. Support three modes: 'sum', 'average', 'median'
3. Filter out non-numeric values
4. Return None if list is empty
5. Raise ValueError if invalid mode

Examples:
- process_data([1, 2, 3, 4, 5], mode='average') â†’ 3.0
- process_data([1, 2, 'a', 3], mode='sum') â†’ 6
- process_data([], mode='sum') â†’ None
"""
```

**ğŸ’¡ LecciÃ³n**: Una descripciÃ³n clara y especÃ­fica es crucial para que el LLM entienda quÃ© hacer.

---

### PASO 2: Definir Test Cases (TDD)

```python
test_cases = [
    # Tests bÃ¡sicos
    {"inputs": ([1, 2, 3, 4, 5], "sum"), "expected": 15},
    {"inputs": ([1, 2, 3, 4, 5], "average"), "expected": 3.0},

    # Edge cases
    {"inputs": ([], "sum"), "expected": None},
    {"inputs": ([1, 2, "a", 3], "sum"), "expected": 6},  # Filtra 'a'
    {"inputs": ([1, 3, 4], "median"), "expected": 3},

    # Error handling
    {"inputs": ([1, 2, 3], "invalid_mode"), "expected": ValueError},
]
```

**ğŸ’¡ LecciÃ³n**: Los tests son tu "criterio de Ã©xito" - definen cuÃ¡ndo el cÃ³digo es correcto.

---

### PASO 3: GeneraciÃ³n Inicial

```python
initial_prompt = f"""
You are an expert Python developer.

{task_description}

Write only the function surrounded by ```python and ``` without explanations.
"""

# LLM genera primera versiÃ³n
response = llm.generate(initial_prompt)
code = extract_code(response)

# Ejecutar tests
results = execute_code(code, test_cases)
# Resultado: Puede que pasen solo 4/12 tests inicialmente
```

**ğŸ’¡ LecciÃ³n**: La primera versiÃ³n rara vez es perfecta - estÃ¡ bien, ese es el punto del loop.

---

### PASO 4: Crear Feedback Estructurado

```python
def format_feedback(results):
    """
    Convierte resultados de tests en feedback legible para el LLM
    """
    feedback = []

    # Si hay error de ejecuciÃ³n
    if results["execution_error"]:
        feedback.append(f"ERROR: {results['execution_error']['error_type']}")
        feedback.append(f"Message: {results['execution_error']['error_message']}")
        return "\n".join(feedback)

    # Resumen
    feedback.append(f"Tests: {results['passed']} passed, {results['failed']} failed")

    # Detalles de tests fallidos
    if results["failed"] > 0:
        feedback.append("\nFailed Test Cases:")
        for test in results["test_results"]:
            if not test["passed"]:
                feedback.append(f"\nTest #{test['test_id']}:")
                feedback.append(f"  Inputs: {test['inputs']}")
                feedback.append(f"  Expected: {test['expected']}")
                feedback.append(f"  Actual: {test['actual']}")

    return "\n".join(feedback)
```

**Ejemplo de feedback generado:**
```
Test Results: 4 passed, 8 failed

Failed Test Cases:

Test #5:
  Inputs: ([], 'sum')
  Expected: None
  Actual: 0

Test #6:
  Inputs: ([1, 2, 'a', 3], 'sum')
  Expected: 6
  Error: unsupported operand type(s) for +: 'int' and 'str'
```

**ğŸ’¡ LecciÃ³n**: Feedback especÃ­fico y estructurado es clave - el LLM necesita saber exactamente quÃ© fallÃ³.

---

### PASO 5: Loop de Mejora Iterativa

```python
iterations = []
current_code = initial_code
current_feedback = initial_feedback

# MÃ¡ximo 3 iteraciones de mejora
for i in range(3):
    # Si todos pasan, terminar
    if all_tests_passed():
        print("âœ… Success! All tests passed.")
        break

    # Crear prompt con feedback
    feedback_prompt = f"""
    You wrote this function:
    ```python
    {current_code}
    ```

    Test results:
    {current_feedback}

    Please improve the code to fix all issues.
    Write only the improved function.
    """

    # LLM mejora el cÃ³digo
    improved_code = llm.generate(feedback_prompt)

    # Ejecutar tests de nuevo
    results = execute_code(improved_code, test_cases)
    feedback = format_feedback(results)

    # Guardar iteraciÃ³n
    iterations.append({
        "iteration": i + 1,
        "code": improved_code,
        "passed": results["passed"],
        "failed": results["failed"]
    })

    # Actualizar para siguiente iteraciÃ³n
    current_code = improved_code
    current_feedback = feedback
```

**Progreso tÃ­pico:**
- IteraciÃ³n 0: 4/12 tests âœ… (implementaciÃ³n bÃ¡sica)
- IteraciÃ³n 1: 8/12 tests âœ… (agrega median, filtra no-numÃ©ricos)
- IteraciÃ³n 2: 11/12 tests âœ… (maneja listas vacÃ­as)
- IteraciÃ³n 3: 12/12 tests âœ… (valida modo invÃ¡lido)

**ğŸ’¡ LecciÃ³n**: Cada iteraciÃ³n se enfoca en los tests que aÃºn fallan, mejorando incrementalmente.

---

## ğŸ”‘ Conceptos Clave del PatrÃ³n

### 1. **Test-Driven Development (TDD)**
- Defines los tests ANTES de escribir el cÃ³digo
- Los tests son tu "norte" - el criterio objetivo de Ã©xito

### 2. **Feedback EspecÃ­fico**
- No solo "estÃ¡ mal" â†’ "Esto fallÃ³ por esta razÃ³n especÃ­fica"
- El LLM puede enfocarse en problemas concretos

### 3. **Mejora Iterativa**
- No esperas perfecciÃ³n en la primera iteraciÃ³n
- Cada ciclo mejora sobre el anterior

### 4. **AutomatizaciÃ³n**
- Todo el proceso puede correr sin intervenciÃ³n humana
- Puedes aplicarlo a mÃºltiples tareas en paralelo

---

## ğŸ“ CÃ³mo Replicar Este PatrÃ³n

### Template GenÃ©rico

```python
def feedback_loop(task_description, test_cases, max_iterations=5):
    """
    Template genÃ©rico para cualquier tarea con feedback loop
    """
    # 1. GeneraciÃ³n inicial
    code = llm.generate(task_description)

    # 2. Loop de mejora
    for i in range(max_iterations):
        # 2a. Ejecutar y evaluar
        results = evaluate(code, test_cases)

        # 2b. Si cumple criterio de Ã©xito, terminar
        if results["success"]:
            return code, results

        # 2c. Generar feedback estructurado
        feedback = format_feedback(results)

        # 2d. Pedir mejora al LLM
        code = llm.improve(code, feedback)

    return code, results
```

### Aplicaciones del PatrÃ³n

1. **GeneraciÃ³n de CÃ³digo**
   - Tests: Unit tests
   - Feedback: Test failures, errores de sintaxis

2. **Escritura de DocumentaciÃ³n**
   - Tests: Criterios de calidad (claridad, completitud)
   - Feedback: Secciones faltantes, ambigÃ¼edades

3. **TraducciÃ³n**
   - Tests: Fluency, accuracy checks
   - Feedback: Errores gramaticales, inconsistencias

4. **AnÃ¡lisis de Datos**
   - Tests: ValidaciÃ³n de resultados esperados
   - Feedback: Discrepancias, errores de cÃ¡lculo

---

## ğŸ“Š Ventajas vs. Desarrollo Tradicional

| Aspecto | Tradicional | Feedback Loop |
|---------|-------------|---------------|
| **Velocidad** | Horas/dÃ­as | Minutos |
| **Iteraciones** | Manual, lenta | AutomÃ¡tica, rÃ¡pida |
| **Cobertura** | Variable | SistemÃ¡tica (todos los tests) |
| **Escalabilidad** | 1 tarea a la vez | N tareas en paralelo |
| **Consistencia** | VarÃ­a por desarrollador | Consistente (basado en tests) |

---

## âš ï¸ Limitaciones y Consideraciones

### CuÃ¡ndo SÃ usar Feedback Loops:
âœ… Tareas bien definidas con criterios claros de Ã©xito
âœ… Tests automÃ¡ticos disponibles
âœ… Problemas que requieren mÃºltiples iteraciones
âœ… AutomatizaciÃ³n de debugging

### CuÃ¡ndo NO usar:
âŒ Tareas creativas sin criterio objetivo
âŒ Problemas que requieren contexto profundo del dominio
âŒ Cuando el feedback no puede ser automatizado
âŒ Tareas Ãºnicas que no justifican el setup

---

## ğŸš€ Ejercicios para Practicar

### Ejercicio 1: SQL Query Generator
Crea un feedback loop que genere queries SQL:
- Task: Generar query para obtener top 10 clientes por ventas
- Tests: Validar sintaxis, verificar resultado con datos de prueba
- Feedback: Errores de SQL, resultados incorrectos

### Ejercicio 2: API Response Parser
- Task: Parsear respuesta JSON de una API
- Tests: Validar campos extraÃ­dos, tipos de datos
- Feedback: Campos faltantes, errores de parsing

### Ejercicio 3: Regex Pattern Generator
- Task: Generar regex para validar emails
- Tests: Casos de emails vÃ¡lidos/invÃ¡lidos
- Feedback: False positives, false negatives

---

## ğŸ“– Recursos Adicionales

- **Test-Driven Development**: Libro "Test Driven Development" by Kent Beck
- **LLM Prompting**: OpenAI Prompt Engineering Guide
- **Code Execution**: Python `exec()` documentation (con precauciones de seguridad)

---

## âœ… Checklist para Tu Propio Feedback Loop

- [ ] Definir tarea claramente (task description)
- [ ] Crear test cases comprehensivos
- [ ] Implementar funciÃ³n de ejecuciÃ³n/evaluaciÃ³n
- [ ] Crear formato de feedback estructurado
- [ ] Implementar loop con condiciÃ³n de salida
- [ ] Agregar logging para anÃ¡lisis
- [ ] Validar con casos de prueba reales
- [ ] Documentar el proceso

---

## ğŸ¯ ConclusiÃ³n

Los **Feedback Loops** transforman la forma en que usamos LLMs:
- De generaciÃ³n "one-shot" â†’ Refinamiento iterativo
- De outputs variables â†’ Resultados consistentes
- De procesos manuales â†’ AutomatizaciÃ³n completa

**El patrÃ³n es simple pero poderoso**: generar â†’ evaluar â†’ retroalimentar â†’ mejorar â†’ repetir.

Â¡Ahora tienes las herramientas para implementar este patrÃ³n en tus propios proyectos! ğŸš€
