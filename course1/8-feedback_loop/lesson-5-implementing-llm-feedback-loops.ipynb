{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Implementing LLM Feedback Loops\n",
    "\n",
    "## Code Generation and Debugging Assistant\n",
    "\n",
    "In this hands-on exercise, you will implement iterative feedback loops where an AI generates, tests, and revises Python code snippets based on test results and feedback.\n",
    "\n",
    "We will use an LLM Feedback loop to create and iteratively improve a Python function called `process_data` that is best described using the following examples:\n",
    "\n",
    "```python\n",
    "process_data([1, 2, 3, 4, 5], mode='average')  # Should return 3.0\n",
    "process_data([1, 2, 'a', 3], mode='sum')  # Should return 6\n",
    "```\n",
    "\n",
    "\n",
    "### Outline:\n",
    "\n",
    "- Setup\n",
    "- Define Task and Test Cases\n",
    "- Initial Generation\n",
    "- Expand the Test Cases\n",
    "- First Iteration with Feedback\n",
    "- Create Feedback Loop\n",
    "- Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import necessary libraries and define helper functions, including a mock LLM client, code execution environment, and test runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# No changes needed in this cell\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import traceback\n",
    "import io\n",
    "import os\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM credentials\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    # Uncomment one of the following\n",
    "    # api_key=\"**********\",  # <--- TODO: Fill in your Vocareum API key here\n",
    "    # api_key=os.getenv(\n",
    "    #     \"OPENAI_API_KEY\"\n",
    "    # ),  # <-- Alternately, set as an environment variable\n",
    ")\n",
    "\n",
    "# If using OpenAI's API endpoint\n",
    "# client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "# No changes needed in this cell\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(messages=None, system_prompt=None, user_prompt=None, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "\n",
    "    messages = list(messages)\n",
    "    if system_prompt:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    if user_prompt:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def execute_code(code, test_cases):\n",
    "    \"\"\"\n",
    "    Executes Python code and returns the results of test cases.\n",
    "    Args:\n",
    "        code: String containing Python code\n",
    "        test_cases: List of dictionaries with inputs and expected outputs\n",
    "    Returns:\n",
    "        Dictionary containing execution results and test outcomes\n",
    "    \"\"\"\n",
    "    results = {\"execution_error\": None, \"test_results\": [], \"passed\": 0, \"failed\": 0}\n",
    "\n",
    "    # Create a namespace for execution\n",
    "    namespace = {}\n",
    "\n",
    "    # Capture stdout and stderr\n",
    "    output_buffer = io.StringIO()\n",
    "\n",
    "    try:\n",
    "        with redirect_stdout(output_buffer), redirect_stderr(output_buffer):\n",
    "            exec(code, namespace)\n",
    "\n",
    "        # Run test cases\n",
    "        for i, test in enumerate(test_cases):\n",
    "            inputs = test[\"inputs\"]\n",
    "            expected = test[\"expected\"]\n",
    "\n",
    "            # Execute the function with test inputs\n",
    "            try:\n",
    "                if isinstance(inputs, dict):\n",
    "                    actual = namespace[\"process_data\"](**inputs)\n",
    "                else:\n",
    "                    actual = namespace[\"process_data\"](*inputs)\n",
    "\n",
    "                passed = actual == expected\n",
    "\n",
    "                if passed:\n",
    "                    results[\"passed\"] += 1\n",
    "                else:\n",
    "                    results[\"failed\"] += 1\n",
    "\n",
    "                results[\"test_results\"].append(\n",
    "                    {\n",
    "                        \"test_id\": i + 1,\n",
    "                        \"inputs\": inputs,\n",
    "                        \"expected\": expected,\n",
    "                        \"actual\": actual,\n",
    "                        \"passed\": passed,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # If the error is the expected type, mark as passed\n",
    "                passed = isinstance(expected, type) and isinstance(e, expected)\n",
    "                results[\"test_results\"].append(\n",
    "                    {\n",
    "                        \"test_id\": i + 1,\n",
    "                        \"inputs\": inputs,\n",
    "                        \"expected\": expected,\n",
    "                        \"error\": str(e),\n",
    "                        \"passed\": passed,\n",
    "                    }\n",
    "                )\n",
    "                if passed:\n",
    "                    results[\"passed\"] += 1\n",
    "                else:\n",
    "                    results[\"failed\"] += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        results[\"execution_error\"] = {\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "        }\n",
    "\n",
    "    results[\"stdout\"] = output_buffer.getvalue()\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to format test results as feedback for the model\n",
    "def format_feedback(results):\n",
    "    \"\"\"\n",
    "    Formats test results into a clear feedback string for the model.\n",
    "    Args:\n",
    "        results: Dictionary containing execution results\n",
    "    Returns:\n",
    "        Formatted feedback string\n",
    "    \"\"\"\n",
    "    feedback = []\n",
    "\n",
    "    if results[\"execution_error\"]:\n",
    "        feedback.append(\n",
    "            f\"ERROR: Code execution failed with {results['execution_error']['error_type']}\"\n",
    "        )\n",
    "        feedback.append(f\"Message: {results['execution_error']['error_message']}\")\n",
    "        feedback.append(\"Traceback:\")\n",
    "        feedback.append(results[\"execution_error\"][\"traceback\"])\n",
    "        feedback.append(\"\\nPlease fix the syntax or runtime errors in the code.\")\n",
    "        return \"\\n\".join(feedback)\n",
    "\n",
    "    feedback.append(\n",
    "        f\"Test Results: {results['passed']} passed, {results['failed']} failed\"\n",
    "    )\n",
    "\n",
    "    if results[\"stdout\"]:\n",
    "        feedback.append(f\"\\nStandard output:\\n{results['stdout']}\")\n",
    "\n",
    "    if results[\"failed\"] > 0:\n",
    "        feedback.append(\"\\nFailed Test Cases:\")\n",
    "        for test in results[\"test_results\"]:\n",
    "            if not test.get(\"passed\"):\n",
    "                feedback.append(f\"\\nTest #{test['test_id']}:\")\n",
    "                feedback.append(f\"  Inputs: {test['inputs']}\")\n",
    "                feedback.append(f\"  Expected: {test['expected']}\")\n",
    "                if \"actual\" in test:\n",
    "                    feedback.append(f\"  Actual: {test['actual']}\")\n",
    "                if \"error\" in test:\n",
    "                    feedback.append(f\"  Error: {test['error']}\")\n",
    "\n",
    "    return \"\\n\".join(feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Task and Test Cases\n",
    "\n",
    "We will create a Python function called `process_data` that analyzes numerical data with the following (possibly incomplete) set of requirements:\n",
    "\n",
    "1. The function should accept a list of numbers and an optional parameter 'mode' that can be 'sum' or 'average' (default should be 'average').\n",
    "2. If mode is 'sum', return the sum of all numbers.\n",
    "3. If mode is 'average', return the average (mean) of all numbers.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "process_data([1, 2, 3, 4, 5], mode='average')  # Should return 3.0\n",
    "process_data([1, 2, 'a', 3], mode='sum')  # Should return 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPLETADO: Descripci√≥n clara de la tarea para el LLM\n# Esta descripci√≥n define exactamente qu√© debe hacer la funci√≥n process_data\ntask_description = \"\"\"\nCreate a Python function called `process_data` that processes a list of values with different modes:\n\nRequirements:\n1. Accept a list as the first parameter and a 'mode' parameter (default: 'average')\n2. Support three modes:\n   - 'sum': Return the sum of all numeric values\n   - 'average': Return the average (mean) of all numeric values\n   - 'median': Return the median of all numeric values\n3. Filter out non-numeric values (ignore strings, None, etc.)\n4. Return None if the list is empty or contains no numeric values\n5. Raise ValueError if an invalid mode is provided\n\nExamples:\n- process_data([1, 2, 3, 4, 5], mode='average') should return 3.0\n- process_data([1, 2, 'a', 3], mode='sum') should return 6 (ignoring 'a')\n- process_data([], mode='sum') should return None\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPLETADO: Casos de prueba iniciales (simples) para validar la funci√≥n\n# Empezamos con casos b√°sicos para que el LLM genere una primera versi√≥n\ntest_cases = [\n    # Test b√°sico de suma\n    {\"inputs\": ([1, 2, 3, 4, 5], \"sum\"), \"expected\": 15},\n    # Test b√°sico de promedio\n    {\"inputs\": ([1, 2, 3, 4, 5], \"average\"), \"expected\": 3.0},\n    # Test con otros n√∫meros para suma\n    {\"inputs\": ([10, 20, 30], \"sum\"), \"expected\": 60},\n    # Test con otros n√∫meros para promedio\n    {\"inputs\": ([2, 4, 6, 8], \"average\"), \"expected\": 5.0},\n]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Generation\n",
    "\n",
    "Let's start with a basic prompt to generate an initial solution to our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPLETADO: Prompt inicial para que el LLM genere la primera versi√≥n del c√≥digo\n# Este prompt es claro y espec√≠fico, pidiendo solo la funci√≥n sin explicaciones\ninitial_prompt = f\"\"\"\nYou are an expert Python developer.\n\n{task_description}\n\nWrite only the function surrounded by ```python and ``` without any additional explanations or examples.\n\nExample format:\n\n```python\ndef process_data(data, mode='average'):\n    # Your implementation here\n    pass\n```\n\"\"\"\n\n# Get initial completion\nmessages = [{\"role\": \"user\", \"content\": initial_prompt}]\ninitial_response = get_completion(messages)\n\n\ndef extract_code(code):\n    \"\"\"Extrae el c√≥digo Python de la respuesta del LLM (entre ```python y ```)\"\"\"\n    lines = code.split(\"\\n\")\n    start = lines.index(\"```python\") + 1\n    end = lines.index(\"```\", start)\n    return \"\\n\".join(lines[start:end])\n\n\n# Extraer el c√≥digo generado\ninitial_code = extract_code(initial_response)\n\nprint(\"Initial Generated Code:\")\nprint(initial_code)\n\n# Ejecutar y probar el c√≥digo inicial\ninitial_results = execute_code(initial_code, test_cases)\ninitial_feedback = format_feedback(initial_results)\n\nprint(\"\\nTest Results:\")\nprint(initial_feedback)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expand the Test Cases\n",
    "\n",
    "Now, pretend that you've used this code in a production setting and have received feedback. The first version of your generated code worked marvelously, and now you are seeking to expand the capabilities of your function.\n",
    "\n",
    "Unfortunately, your product manager is on vacation, but you have know your function needs to:\n",
    "1) support a new mode, \"median\"\n",
    "2) ignore non-numeric values\n",
    "3) handle empty lists, returning None\n",
    "\n",
    "So, following test-driven development practices, you update your tests:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the new test cases. No updates needed.\n",
    "test_cases = [\n",
    "    {\"inputs\": ([1, 2, 3, 4, 5], \"sum\"), \"expected\": 15},\n",
    "    {\"inputs\": ([1, 2, 3, 4, 5], \"average\"), \"expected\": 3.0},\n",
    "    {\"inputs\": ([11, 12, 13, 14, 15], \"sum\"), \"expected\": 65},\n",
    "    {\"inputs\": ([11, 12, 13, 14, 15], \"average\"), \"expected\": 13.0},\n",
    "    {\"inputs\": ([], \"sum\"), \"expected\": None},\n",
    "    {\"inputs\": ([1, 3, 4], \"median\"), \"expected\": 3},\n",
    "    {\"inputs\": ([1, 2, 3, 5], \"median\"), \"expected\": 2.5},\n",
    "    {\"inputs\": ([1, 2, \"a\", 3], \"sum\"), \"expected\": 6},\n",
    "    {\"inputs\": ([1, 2, None, 3, \"b\", 4], \"average\"), \"expected\": 2.5},\n",
    "    {\"inputs\": ([10], \"median\"), \"expected\": 10},\n",
    "    {\"inputs\": ([], \"median\"), \"expected\": None},\n",
    "    {\"inputs\": ([1, 2, 3, 4, 5], \"invalid_mode\"), \"expected\": ValueError},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-test the code\n",
    "# No updates are needed in this cell\n",
    "print(\"Initial Generated Code:\")\n",
    "print(initial_code)\n",
    "\n",
    "# Execute and test the initial code\n",
    "initial_results = execute_code(initial_code, test_cases)\n",
    "initial_feedback = format_feedback(initial_results)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(initial_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. First Iteration with Feedback\n",
    "Now, let's feed the test results back to the model and ask for an improved version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPLETADO: Primera iteraci√≥n con feedback\n# Aqu√≠ le mostramos al LLM los resultados de las pruebas y le pedimos que mejore\nfeedback_prompt = f\"\"\"\nYou are an expert Python developer. You wrote a function based on these requirements:\n\n{task_description}\n\nHere is your current implementation:\n```python\n{initial_code}\n```\n\nI've tested your code and here are the results:\n{initial_feedback}\n\nPlease improve your code to fix any issues and make all tests pass.\nWrite only the improved function surrounded by ```python and ``` without any explanations.\n\"\"\"\n\nmessages = [{\"role\": \"user\", \"content\": feedback_prompt}]\n\n# Get improved code\nimproved_response = get_completion(messages)\n\n# Extract the improved code\nimproved_code = extract_code(improved_response)\n\nprint(\"\\nImproved Code:\")\nprint(improved_code)\n\n# Execute and test the improved code\nimproved_results = execute_code(improved_code, test_cases)\nimproved_feedback = format_feedback(improved_results)\nprint(\"\\nTest Results for Improved Code:\")\nprint(improved_feedback)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Feedback Loop\n",
    "\n",
    "We may want to give the LLM more than one chance to generate the correct code. We may even want to introduce test cases gradually, so that it has the opportunity to fix errors one at a time.\n",
    "\n",
    "Let's develop a loop that will start from scratch and run the loop a maximum number of times or until the code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPLETADO: Loop de feedback completo - Itera hasta que todos los tests pasen\n# Este es el patr√≥n de feedback loop: generar ‚Üí probar ‚Üí retroalimentar ‚Üí mejorar\nfrom pprint import pprint\n\niterations = []\n\n# ====================\n# PASO 1: Generaci√≥n inicial\n# ====================\n# Get initial completion and extract code\nmessages = [{\"role\": \"user\", \"content\": initial_prompt}]\ninitial_response = get_completion(messages)\ninitial_code = extract_code(initial_response)\n\n# Execute and test the initial code\ninitial_results = execute_code(initial_code, test_cases)\ninitial_feedback = format_feedback(initial_results)\n\n# Store the initial iteration\niterations.append(\n    {\n        \"iteration\": 0,\n        \"code\": initial_code,\n        \"test_results\": {\n            \"passed\": initial_results[\"passed\"],\n            \"failed\": initial_results[\"failed\"],\n        },\n    }\n)\n\nprint(\"=== ITERATION 0 (Initial Generation) ===\")\npprint(iterations[-1][\"test_results\"])\n\ncurrent_code = initial_code\ncurrent_feedback = initial_feedback\n\n# ====================\n# PASO 2: Loop de mejora iterativa\n# ====================\n# Loop to improve the code based on feedback\nfor i in range(3):  # M√°ximo 3 iteraciones de mejora\n    # Si todos los tests pasan, salimos del loop\n    if iterations[-1][\"test_results\"][\"failed\"] == 0:\n        print(\"\\n‚úÖ Success! All tests passed.\")\n        break\n    \n    print(f\"\\n=== ITERATION {i+1} (Improvement) ===\")\n    \n    # Crear el prompt de feedback con el c√≥digo actual y los resultados\n    feedback_prompt = f\"\"\"\nYou are an expert Python developer. You wrote a function based on these requirements:\n\n{task_description}\n\nHere is your current implementation:\n```python\n{current_code}\n```\n\nI've tested your code and here are the results:\n{current_feedback}\n\nPlease improve your code to fix any issues and make sure it passes all test cases.\nWrite only the improved function surrounded by ```python and ``` without any explanation.\n\"\"\"\n\n    # Obtener c√≥digo mejorado del LLM\n    messages = [{\"role\": \"user\", \"content\": feedback_prompt}]\n    improved_response = get_completion(messages)\n    improved_code = extract_code(improved_response)\n\n    # Execute and test the improved code\n    improved_results = execute_code(improved_code, test_cases)\n    improved_feedback = format_feedback(improved_results)\n    \n    # Guardar esta iteraci√≥n\n    iterations.append(\n        {\n            \"iteration\": i + 1,\n            \"code\": improved_code,\n            \"test_results\": {\n                \"passed\": improved_results[\"passed\"],\n                \"failed\": improved_results[\"failed\"],\n            },\n        }\n    )\n    pprint(iterations[-1][\"test_results\"])\n\n    # Actualizar para la siguiente iteraci√≥n\n    current_code = improved_code\n    current_feedback = improved_feedback\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FEEDBACK LOOP COMPLETED\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a summary of the different iterations\n",
    "from pprint import pprint\n",
    "pprint(iterations, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final code\n",
    "print(iterations[-1][\"code\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Reflection & Transfer\n\n### üìä AN√ÅLISIS DE MEJORAS POR ITERACI√ìN\n\n**Observaciones sobre las iteraciones:**\n\n1. **Correcci√≥n (Correctness)**:\n   - ¬øAument√≥ el n√∫mero de tests que pasan en cada iteraci√≥n?\n   - El feedback loop permite que el LLM aprenda de sus errores espec√≠ficos\n   \n2. **Manejo de errores (Error handling)**:\n   - Primera iteraci√≥n: C√≥digo b√°sico, puede fallar con edge cases\n   - Iteraciones posteriores: Agrega validaciones y manejo de excepciones\n   \n3. **Casos extremos (Edge cases)**:\n   - Lista vac√≠a ‚Üí Debe retornar None\n   - Valores no num√©ricos ‚Üí Deben ser filtrados\n   - Modo inv√°lido ‚Üí Debe levantar ValueError\n   \n4. **Legibilidad (Readability)**:\n   - El c√≥digo se vuelve m√°s robusto y documentado\n   - Se agregan validaciones claras\n\n---\n\n### üîÑ EFECTIVIDAD DEL FEEDBACK LOOP\n\n**Ventajas del approach:**\n\n‚úÖ **Iterativo y espec√≠fico**: El LLM recibe feedback detallado sobre qu√© fall√≥\n‚úÖ **Aprendizaje progresivo**: Cada iteraci√≥n mejora sobre la anterior\n‚úÖ **Automatizable**: Este patr√≥n se puede aplicar a cualquier tarea de generaci√≥n de c√≥digo\n‚úÖ **Test-driven**: Usa TDD (Test-Driven Development) como gu√≠a\n\n**Tipos de problemas resueltos por iteraci√≥n:**\n- Iteraci√≥n 0: Implementaci√≥n b√°sica (sum/average)\n- Iteraci√≥n 1: Agregar modo 'median', filtrar no-num√©ricos\n- Iteraci√≥n 2: Manejar listas vac√≠as, validar modo inv√°lido\n- Iteraci√≥n 3: Refinamientos finales\n\n**Problemas persistentes:**\n- A veces el LLM puede \"sobrecomplicar\" la soluci√≥n\n- Algunos edge cases muy espec√≠ficos pueden requerir m√°s iteraciones\n\n---\n\n### üí° LECCIONES CLAVE PARA REPLICAR\n\n**Patr√≥n de Feedback Loop para Code Generation:**\n\n```python\n# 1. DEFINIR TASK + TEST CASES\ntask_description = \"...\"\ntest_cases = [...]\n\n# 2. GENERAR C√ìDIGO INICIAL\ncode = llm.generate(task_description)\n\n# 3. LOOP DE MEJORA\nfor iteration in range(max_iterations):\n    # 3a. Ejecutar y obtener resultados\n    results = execute_and_test(code, test_cases)\n    \n    # 3b. Si todos pasan, terminar\n    if all_tests_passed(results):\n        break\n    \n    # 3c. Generar feedback estructurado\n    feedback = format_feedback(results)\n    \n    # 3d. Pedir al LLM que mejore\n    code = llm.improve(code, feedback)\n```\n\n**Mejoras potenciales:**\n1. Agregar m√°s contexto en el feedback (ej: traceback completo)\n2. Usar diferentes temperaturas (m√°s baja = m√°s determinista)\n3. Implementar \"reflexi√≥n\" donde el LLM explica qu√© cambi√≥ y por qu√©\n4. Guardar todas las versiones del c√≥digo para an√°lisis\n\n---\n\n### üÜö COMPARACI√ìN CON ENFOQUES TRADICIONALES\n\n| Enfoque | Tradicional | LLM Feedback Loop |\n|---------|-------------|-------------------|\n| **Debugging** | Manual, l√≠nea por l√≠nea | Automatizado, basado en tests |\n| **Iteraciones** | Lentas (humano escribe c√≥digo) | R√°pidas (LLM genera en segundos) |\n| **Cobertura** | Depende del desarrollador | Sistem√°tica (todos los test cases) |\n| **Aprendizaje** | Experiencia acumulada | Feedback inmediato y espec√≠fico |\n| **Escalabilidad** | Limitada por tiempo humano | Alta (m√∫ltiples tareas en paralelo) |\n\n**Cu√°ndo usar Feedback Loops:**\n- ‚úÖ Tareas bien definidas con tests claros\n- ‚úÖ Problemas que requieren m√∫ltiples iteraciones\n- ‚úÖ Automatizaci√≥n de debugging y refinamiento\n- ‚úÖ Generaci√≥n de c√≥digo con requisitos complejos\n\n**Cu√°ndo NO usar:**\n- ‚ùå Tareas creativas sin criterios claros de √©xito\n- ‚ùå Problemas que requieren contexto humano profundo\n- ‚ùå Cuando el feedback no puede ser automatizado"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, we explored how LLM feedback loops can be used to iteratively improve code generation.\n",
    "\n",
    "By providing structured feedback about test failures, we enabled the model to focus on specific issues and incrementally improve its solution.\n",
    "\n",
    "The key insight is that well-structured feedback loops can significantly enhance the quality and correctness of AI-generated code, especially for complex tasks with multiple (possibly incomplete) requirements and edge cases.\n",
    "\n",
    "\n",
    "Congratulations on completing this exercise! Give yourself a hand! ü§óü§ó"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai_udacity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}