{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Chaining Prompts for Agentic Reasoning\n",
    "\n",
    "## Automated Claim Triage: From First-Notice to the Right Queue\n",
    "\n",
    "In this hands-on exercise, you will build a prompt chain that extracts key fields from free-form auto-claim reports, assesses damage severity, and routes each claim to one of several queues ‚Äî `glass`, `fast_track`, `material_damage`, or `total_loss` ‚Äî with code-based gate checks at every step.\n",
    "\n",
    "## Outline:\n",
    "\n",
    "- Setup\n",
    "- Sample FNOL (First Notice of Loss) Texts\n",
    "- Stage I: Information Extraction\n",
    "- Stage II: Severity Assessment\n",
    "- Stage III: Queue Routing\n",
    "- Review Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## üìù RESUMEN DE COMPLETACI√ìN Y CORRECCIONES\n\n### ‚úÖ Qu√© se complet√≥ en este notebook:\n\nEste notebook implementa un **sistema de cadena de prompts (prompt chaining)** para triaje automatizado de claims de seguros de auto. El sistema procesa reportes FNOL (First Notice of Loss) en texto libre y los enruta a colas de procesamiento.\n\n### üîß Correcciones realizadas por Claude:\n\n1. **Cell 9 - ClaimInformation**:\n   - ‚úÖ Completada la clase Pydantic con todos los campos requeridos\n   - ‚úÖ Corregido `min_items` ‚Üí `min_length` (deprecado en Pydantic V2)\n   - ‚úÖ Completado el prompt `info_extraction_system_prompt`\n   - ‚úÖ Mejorado el prompt para evitar valores inv√°lidos en `damage_area`\n\n2. **Cell 10 - Gate Check 1**:\n   - ‚úÖ Completada la funci√≥n `extract_claim_info` con validaci√≥n Gate 1\n   - ‚úÖ Agregados comentarios explicativos en espa√±ol\n\n3. **Cell 13 - SeverityAssessment**:\n   - ‚úÖ Completado el prompt `severity_assessment_system_prompt`\n   - ‚úÖ Agregados comentarios sobre los rangos de costo\n\n4. **Cell 14 - Gate Check 2**:\n   - ‚úÖ Completadas las validaciones de rango de costos para Minor ($100-$1000), Moderate ($1000-$5000), Major ($5000-$50000)\n   - ‚úÖ Agregado manejo de `None` para prevenir AttributeError\n   - ‚úÖ Agregados comentarios explicativos\n\n5. **Cell 17 - ClaimRouting**:\n   - ‚úÖ Completado el prompt `queue_routing_system_prompt`\n   - ‚úÖ Agregadas reglas de enrutamiento claramente definidas\n\n6. **Cell 18 - Gate Check 3**:\n   - ‚úÖ Completado el diccionario `routing_input` con toda la informaci√≥n necesaria\n   - ‚úÖ Agregado manejo de `None` para claim_info y severity_assessment\n   - ‚úÖ Agregados comentarios explicativos\n\n7. **Cell 21 - DataFrame de resultados**:\n   - ‚úÖ Agregado manejo robusto de valores `None`\n   - ‚úÖ Agregado resumen de claims exitosos vs fallidos\n   - ‚úÖ Uso correcto de `.model_dump()` para Pydantic V2\n\n### üéØ C√≥mo funciona el sistema:\n\n**STAGE 1: Extracci√≥n de Informaci√≥n**\n- Extrae datos estructurados del texto FNOL\n- Gate Check 1 valida el JSON y los tipos de datos\n\n**STAGE 2: Evaluaci√≥n de Severidad**\n- Eval√∫a la severidad del da√±o (Minor/Moderate/Major)\n- Estima el costo de reparaci√≥n\n- Gate Check 2 valida que el costo est√© en el rango correcto\n\n**STAGE 3: Enrutamiento a Colas**\n- Determina la cola apropiada bas√°ndose en severidad y tipo de da√±o\n- Colas: `glass`, `fast_track`, `material_damage`, `total_loss`\n- Gate Check 3 valida que la cola sea v√°lida\n\n### üõ°Ô∏è Gate Checks (Validaciones de C√≥digo):\n\nLos **gate checks** son validaciones de c√≥digo que previenen la cascada de errores:\n- Si un LLM devuelve datos inv√°lidos, el gate check los detecta\n- Previene que datos incorrectos pasen al siguiente stage\n- Permite manejo elegante de errores sin romper el pipeline\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries and define helper functions, including a mock LLM client, code execution environment, and test runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# No changes needed in this cell\n",
    "from openai import OpenAI  # For accessing the OpenAI API\n",
    "from enum import Enum\n",
    "import json\n",
    "from pydantic import BaseModel, Field  # For structured data validation\n",
    "from typing import List, Literal, Optional\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM credentials\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    # Uncomment one of the following\n",
    "    # api_key=\"**********\",  # <--- TODO: Fill in your Vocareum API key here\n",
    "    # api_key=os.getenv(\n",
    "    #     \"OPENAI_API_KEY\"\n",
    "    # ),  # <-- Alternately, set as an environment variable\n",
    ")\n",
    "\n",
    "# If using OpenAI's API endpoint\n",
    "# client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "# No changes needed in this cell\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(messages=None, system_prompt=None, user_prompt=None, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "\n",
    "    messages = list(messages)\n",
    "    if system_prompt:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    if user_prompt:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample FNOL (First Notice of Loss) Texts\n",
    "Let's define a few sample First Notice of Loss (FNOL) texts to process through our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample FNOL texts\n",
    "# TODO: [Optional] Add more sample FNOL texts to test various scenarios\n",
    "\n",
    "sample_fnols = [\n",
    "    \"\"\"\n",
    "    Claim ID: C001\n",
    "    Customer: John Smith\n",
    "    Vehicle: 2018 Toyota Camry\n",
    "    Incident: While driving on the highway, a rock hit my windshield and caused a small chip\n",
    "    about the size of a quarter. No other damage was observed.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Claim ID: C002\n",
    "    Customer: Sarah Johnson\n",
    "    Vehicle: 2020 Honda Civic\n",
    "    Incident: I was parked at the grocery store and returned to find someone had hit my car and\n",
    "    dented the rear bumper and taillight. The taillight is broken and the bumper has a large dent.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Claim ID: C003\n",
    "    Customer: Michael Rodriguez\n",
    "    Vehicle: 2022 Ford F-150\n",
    "    Incident: I was involved in a serious collision at an intersection. The front of my truck is\n",
    "    severely damaged, including the hood, bumper, radiator, and engine compartment. The airbags\n",
    "    deployed and the vehicle is not drivable.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Claim ID: C004\n",
    "    Customer: Emma Williams\n",
    "    Vehicle: 2019 Subaru Outback\n",
    "    Incident: My car was damaged in a hailstorm. There are multiple dents on the hood, roof, and\n",
    "    trunk. The side mirrors were also damaged and one window has a small crack.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Claim ID: C005\n",
    "    Customer: David Brown\n",
    "    Vehicle: 2021 Tesla Model 3\n",
    "    Incident: Someone keyed my car in the parking lot. There are deep scratches along both doors\n",
    "    on the driver's side.\n",
    "    \"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage I: Information Extraction\n",
    "In this stage, we'll create a prompt that extracts structured information from free-form FNOL text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# STAGE I: EXTRACCI√ìN DE INFORMACI√ìN\n# ========================================\n# PROP√ìSITO: Extraer datos estructurados de reportes FNOL en texto libre\n# COMPLETADO POR CLAUDE: Se complet√≥ la clase ClaimInformation y el prompt de extracci√≥n\n\nclass ClaimInformation(BaseModel):\n    \"\"\"\n    Modelo Pydantic que define la estructura de informaci√≥n de un claim.\n    Valida que los datos extra√≠dos cumplan con los requisitos:\n    - claim_id: ID √∫nico del claim (2-10 caracteres)\n    - name: Nombre completo del cliente (2-100 caracteres)\n    - vehicle: Descripci√≥n del veh√≠culo (2-100 caracteres)\n    - loss_desc: Descripci√≥n del incidente (10-500 caracteres)\n    - damage_area: Lista de √°reas da√±adas (m√≠nimo 1 √°rea)\n    \"\"\"\n    claim_id: str = Field(..., min_length=2, max_length=10)\n    name: str = Field(..., min_length=2, max_length=100)\n    vehicle: str = Field(..., min_length=2, max_length=100)\n    loss_desc: str = Field(..., min_length=10, max_length=500)\n    damage_area: List[\n        Literal[\n            \"windshield\",\n            \"front\",\n            \"rear\",\n            \"side\",\n            \"roof\",\n            \"hood\",\n            \"door\",\n            \"bumper\",\n            \"fender\",\n            \"quarter panel\",\n            \"trunk\",\n            \"glass\",\n        ]\n    ] = Field(..., min_length=1)  # CORREGIDO: Cambiado de min_items a min_length\n\n\ninfo_extraction_system_prompt = \"\"\"\nYou are an expert insurance claims processor. Your task is to extract key information from First Notice of Loss (FNOL) reports.\n\nIMPORTANT: For damage_area, you MUST ONLY use these exact values:\n- windshield, front, rear, side, roof, hood, door, bumper, fender, quarter panel, trunk, glass\n\nDo NOT invent new values or use variations (e.g., \"taillight\" should map to \"rear\" or \"glass\", not \"light\").\n\nFormat your response as a valid JSON object with the following keys:\n- claim_id (str): The claim ID\n- name (str): The customer's full name\n- vehicle (str): The vehicle make, model, and year\n- loss_desc (str): A description of the loss or incident\n- damage_area (list): A list of damaged areas using ONLY the allowed values above\n\nExtract the information accurately from the FNOL text. For damage_area, identify all applicable areas mentioned in the incident description.\n\nOnly respond with the JSON object, nothing else.\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# GATE CHECK 1 y FUNCI√ìN DE EXTRACCI√ìN\n# ========================================\n# COMPLETADO POR CLAUDE: Se complet√≥ la funci√≥n extract_claim_info con validaci√≥n Gate 1\n\ndef gate1_validate_claim_info(claim_info_json: str) -> ClaimInformation:\n    \"\"\"\n    GATE CHECK 1: Valida la informaci√≥n extra√≠da del claim\n    \n    PROP√ìSITO: Asegurar que el LLM devolvi√≥ JSON v√°lido con todos los campos requeridos\n    y que cumplen con las restricciones de Pydantic (longitudes, tipos, valores literales)\n    \n    Returns:\n        ClaimInformation validado\n    Raises:\n        ValueError si la validaci√≥n falla\n    \"\"\"\n    try:\n        # Parse the JSON string\n        claim_info_dict = json.loads(claim_info_json)\n        # Validate with Pydantic model\n        validated_info = ClaimInformation(**claim_info_dict)\n        return validated_info\n    except Exception as e:\n        raise ValueError(f\"Gate 1 validation failed: {str(e)}\")\n\n\ndef extract_claim_info(fnol_text):\n    \"\"\"\n    STAGE 1: Extraer informaci√≥n estructurada del texto FNOL\n    \n    FLUJO:\n    1. Env√≠a el texto FNOL al LLM con el prompt de extracci√≥n\n    2. Recibe respuesta JSON del LLM\n    3. Valida la respuesta con Gate Check 1\n    4. Retorna ClaimInformation validado o None si falla\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": info_extraction_system_prompt},\n        {\"role\": \"user\", \"content\": fnol_text},\n    ]\n\n    response = get_completion(messages=messages)\n\n    # Gate check: validate the extracted information\n    try:\n        validated_info = gate1_validate_claim_info(response)  # <-- COMPLETADO: Validaci√≥n Gate 1\n        return validated_info\n    except ValueError as e:\n        print(f\"Gate 1 failed: {e}\")\n        return None"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate 1 failed: Gate 1 validation failed: 1 validation error for ClaimInformation\n",
      "damage_area.2\n",
      "  Input should be 'windshield', 'front', 'rear', 'side', 'roof', 'hood', 'door', 'bumper', 'fender', 'quarter panel', 'trunk' or 'glass' [type=literal_error, input_value='light', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/literal_error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ClaimInformation(claim_id='C001', name='John Smith', vehicle='2018 Toyota Camry', loss_desc='While driving on the highway, a rock hit my windshield and caused a small chip about the size of a quarter. No other damage was observed.', damage_area=['windshield']),\n",
       " None,\n",
       " ClaimInformation(claim_id='C003', name='Michael Rodriguez', vehicle='2022 Ford F-150', loss_desc='Serious collision at an intersection with severe damage to the front, including hood, bumper, radiator, and engine compartment. Airbags deployed, vehicle not drivable.', damage_area=['front', 'hood', 'bumper']),\n",
       " ClaimInformation(claim_id='C004', name='Emma Williams', vehicle='2019 Subaru Outback', loss_desc='My car was damaged in a hailstorm. There are multiple dents on the hood, roof, and trunk. The side mirrors were also damaged and one window has a small crack.', damage_area=['hood', 'roof', 'trunk', 'side', 'glass']),\n",
       " ClaimInformation(claim_id='C005', name='David Brown', vehicle='2021 Tesla Model 3', loss_desc=\"Someone keyed my car in the parking lot. There are deep scratches along both doors on the driver's side.\", damage_area=['door', 'side'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the claim extraction function on the sample FNOLs\n",
    "# No updates needed in this cell\n",
    "\n",
    "extracted_claim_info_items = [\n",
    "    extract_claim_info(fnol_text) for fnol_text in sample_fnols\n",
    "]\n",
    "extracted_claim_info_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage II: Severity Assessment\n",
    "In this stage, we'll assess the severity of the damage based on the extracted information.\n",
    "\n",
    "Note, our carrier applies the following heuristics:\n",
    "- Minor damage: Small dents, scratches, glass chips (cost range: $100-$1,000)\n",
    "- Moderate damage: Single panel damage, bumper replacement, door damage (cost range: $1,000-$5,000)\n",
    "- Major damage: Structural damage, multiple panel replacement, engine/drivetrain issues, total loss candidates (cost range: $5,000-$50,000)\n",
    "\n",
    "In this example we will let the LLM estimate the cost, though in production we would want a more accurate estimate, e.g. querying a database of repair costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# STAGE II: EVALUACI√ìN DE SEVERIDAD\n# ========================================\n# PROP√ìSITO: Evaluar la severidad del da√±o y estimar el costo de reparaci√≥n\n# COMPLETADO POR CLAUDE: Se complet√≥ el prompt de evaluaci√≥n de severidad\n\nclass SeverityAssessment(BaseModel):\n    \"\"\"\n    Modelo Pydantic para la evaluaci√≥n de severidad del da√±o\n    \n    - severity: Nivel de severidad (Minor, Moderate, Major)\n    - est_cost: Costo estimado de reparaci√≥n en d√≥lares (debe ser > 0)\n    \"\"\"\n    severity: Literal[\"Minor\", \"Moderate\", \"Major\"]\n    est_cost: float = Field(..., gt=0)\n\n\nseverity_assessment_system_prompt = \"\"\"\nYou are an auto insurance damage assessor. Your task is to evaluate the severity of vehicle damage and estimate repair costs.\n\nBased on the claim information provided, assess the severity and estimated repair cost:\n- Minor: Small dents, scratches, glass chips (cost range: $100-$1,000)\n- Moderate: Single panel damage, bumper replacement, door damage (cost range: $1,000-$5,000)\n- Major: Structural damage, multiple panel replacement, engine/drivetrain issues, total loss candidates (cost range: $5,000-$50,000)\n\nIMPORTANT: Your estimated cost (est_cost) MUST fall within the appropriate range for the severity level you assign.\n\nFormat your response as a valid JSON object with the following keys:\n- severity (str): Must be one of \"Minor\", \"Moderate\", or \"Major\"\n- est_cost (float): Estimated repair cost in dollars (must be within the range for the severity)\n\nOnly respond with the JSON object, nothing else.\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# GATE CHECK 2 y FUNCI√ìN DE EVALUACI√ìN DE SEVERIDAD\n# ========================================\n# COMPLETADO POR CLAUDE: Se completaron las validaciones de rango de costos\n\ndef gate2_cost_range_ok(severity_json: str) -> SeverityAssessment:\n    \"\"\"\n    GATE CHECK 2: Valida que el costo estimado est√© dentro del rango correcto para la severidad\n    \n    PROP√ìSITO: Prevenir que el LLM asigne costos inconsistentes con la severidad\n    (ej: \"Minor\" con costo de $10,000)\n    \n    RANGOS VALIDADOS:\n    - Minor: $100 - $1,000\n    - Moderate: $1,000 - $5,000\n    - Major: $5,000 - $50,000\n    \n    Returns:\n        SeverityAssessment validado\n    Raises:\n        ValueError si el costo est√° fuera del rango\n    \"\"\"\n    try:\n        # Parse the JSON string\n        severity_dict = json.loads(severity_json)\n        # Validate with Pydantic model\n        validated_severity = SeverityAssessment(**severity_dict)\n\n        # Check cost range based on severity\n        if (\n            validated_severity.severity == \"Minor\"\n            and (\n                validated_severity.est_cost < 100 or validated_severity.est_cost > 1000\n            )\n        ):\n            raise ValueError(\n                f\"Minor damage should cost between $100-$1000, got ${validated_severity.est_cost}\"\n            )\n        elif (\n            validated_severity.severity == \"Moderate\"\n            and (\n                validated_severity.est_cost < 1000 or validated_severity.est_cost > 5000\n            )\n        ):\n            raise ValueError(\n                f\"Moderate damage should cost between $1000-$5000, got ${validated_severity.est_cost}\"\n            )\n        elif (\n            validated_severity.severity == \"Major\"\n            and (\n                validated_severity.est_cost < 5000 or validated_severity.est_cost > 50000\n            )\n        ):\n            raise ValueError(\n                f\"Major damage should cost between $5000-$50000, got ${validated_severity.est_cost}\"\n            )\n\n        return validated_severity\n    except Exception as e:\n        raise ValueError(f\"Gate 2 validation failed: {str(e)}\")\n\n\ndef assess_severity(claim_info: ClaimInformation) -> Optional[SeverityAssessment]:\n    \"\"\"\n    STAGE 2: Evaluar la severidad del da√±o bas√°ndose en la informaci√≥n del claim\n    \n    FLUJO:\n    1. Convierte ClaimInformation a JSON\n    2. Env√≠a al LLM para evaluaci√≥n de severidad\n    3. Valida respuesta con Gate Check 2\n    4. Retorna SeverityAssessment validado o None si falla\n    \n    Args:\n        claim_info: Informaci√≥n validada del claim (o None si Stage 1 fall√≥)\n    Returns:\n        SeverityAssessment o None\n    \"\"\"\n    # CORREGIDO: Manejo de None - si el claim no se extrajo correctamente, retornar None\n    if claim_info is None:\n        return None\n    \n    # Convert Pydantic model to JSON string\n    claim_info_json = claim_info.model_dump_json()\n\n    messages = [\n        {\"role\": \"system\", \"content\": severity_assessment_system_prompt},\n        {\"role\": \"user\", \"content\": claim_info_json},\n    ]\n\n    response = get_completion(messages=messages)\n\n    # Gate check: validate the severity assessment\n    try:\n        validated_severity = gate2_cost_range_ok(response)\n        return validated_severity\n    except ValueError as e:\n        print(f\"Gate 2 failed: {e}. Response: {response}\")\n        return None"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'model_dump_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the claim extraction function on the sample data\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# No updates needed in this cell\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m severity_assessment_items = \u001b[43m[\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43massess_severity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mextracted_claim_info_items\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m severity_assessment_items\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the claim extraction function on the sample data\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# No updates needed in this cell\u001b[39;00m\n\u001b[32m      4\u001b[39m severity_assessment_items = [\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43massess_severity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m extracted_claim_info_items\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      8\u001b[39m severity_assessment_items\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36massess_severity\u001b[39m\u001b[34m(claim_info)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03mStage 2: Assess severity based on damage description\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Convert Pydantic model to JSON string\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m claim_info_json = \u001b[43mclaim_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump_json\u001b[49m()\n\u001b[32m     60\u001b[39m messages = [\n\u001b[32m     61\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: severity_assessment_system_prompt},\n\u001b[32m     62\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: claim_info_json},\n\u001b[32m     63\u001b[39m ]\n\u001b[32m     65\u001b[39m response = get_completion(messages=messages)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'model_dump_json'"
     ]
    }
   ],
   "source": [
    "# Run the claim extraction function on the sample data\n",
    "# No updates needed in this cell\n",
    "\n",
    "severity_assessment_items = [\n",
    "    assess_severity(item) for item in extracted_claim_info_items\n",
    "]\n",
    "\n",
    "severity_assessment_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage III: Queue Routing\n",
    "In this stage, we'll route the claim to the appropriate queue based on severity and damage area.\n",
    "\n",
    "Use these routing rules:\n",
    "- 'glass' queue: For Minor damage involving ONLY glass (windshield, windows)\n",
    "- 'fast_track' queue: For other Minor damage\n",
    "- 'material_damage' queue: For all Moderate damage\n",
    "- 'total_loss' queue: For all Major damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# STAGE III: ENRUTAMIENTO A COLAS\n# ========================================\n# PROP√ìSITO: Determinar la cola de procesamiento apropiada bas√°ndose en severidad y tipo de da√±o\n# COMPLETADO POR CLAUDE: Se complet√≥ el prompt de enrutamiento\n\nclass ClaimRouting(BaseModel):\n    \"\"\"\n    Modelo Pydantic para el enrutamiento del claim a la cola apropiada\n    \n    - claim_id: ID del claim\n    - queue: Cola asignada (glass, fast_track, material_damage, total_loss)\n    \"\"\"\n    claim_id: str\n    queue: Literal[\"glass\", \"fast_track\", \"material_damage\", \"total_loss\"]\n\n\nqueue_routing_system_prompt = \"\"\"\nYou are an auto insurance claim routing specialist. Your task is to determine the appropriate processing queue for each claim.\n\nROUTING RULES (MUST FOLLOW STRICTLY):\n1. 'glass' queue: For Minor damage involving ONLY glass (windshield, windows)\n2. 'fast_track' queue: For other Minor damage (not glass-only)\n3. 'material_damage' queue: For ALL Moderate damage\n4. 'total_loss' queue: For ALL Major damage\n\nIMPORTANT: Analyze both the severity level AND the damage_area list to determine the correct queue.\nFor glass queue, the damage must be Minor AND only affect glass areas (windshield, glass).\n\nFormat your response as a valid JSON object with the following keys:\n- claim_id (str): The claim ID from the claim information\n- queue (str): Must be one of \"glass\", \"fast_track\", \"material_damage\", or \"total_loss\"\n\nOnly respond with the JSON object, nothing else.\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# GATE CHECK 3 y FUNCI√ìN DE ENRUTAMIENTO\n# ========================================\n# COMPLETADO POR CLAUDE: Se complet√≥ la funci√≥n route_claim\n\ndef gate3_validate_routing(routing_json: str) -> ClaimRouting:\n    \"\"\"\n    GATE CHECK 3: Valida que el claim se haya enrutado a una cola v√°lida\n    \n    PROP√ìSITO: Asegurar que el LLM devolvi√≥ un JSON v√°lido con claim_id y \n    una cola v√°lida (glass, fast_track, material_damage, total_loss)\n    \n    Returns:\n        ClaimRouting validado\n    Raises:\n        ValueError si la validaci√≥n falla\n    \"\"\"\n    try:\n        # Parse the JSON string\n        routing_dict = json.loads(routing_json)\n        # Validate with Pydantic model\n        validated_routing = ClaimRouting(**routing_dict)\n        return validated_routing\n    except Exception as e:\n        raise ValueError(f\"Gate 3 validation failed: {str(e)}\")\n\n\ndef route_claim(\n    claim_info: ClaimInformation, severity_assessment: Optional[SeverityAssessment]\n) -> Optional[ClaimRouting]:\n    \"\"\"\n    STAGE 3: Enrutar el claim a la cola de procesamiento apropiada\n    \n    FLUJO:\n    1. Verifica que tengamos informaci√≥n v√°lida (claim_info y severity_assessment)\n    2. Combina toda la informaci√≥n en un diccionario\n    3. Env√≠a al LLM para determinar la cola apropiada\n    4. Valida respuesta con Gate Check 3\n    5. Retorna ClaimRouting validado o None si falla\n    \n    Args:\n        claim_info: Informaci√≥n del claim (o None si Stage 1 fall√≥)\n        severity_assessment: Evaluaci√≥n de severidad (o None si Stage 2 fall√≥)\n    Returns:\n        ClaimRouting o None\n    \"\"\"\n    # CORREGIDO: Si cualquier stage previo fall√≥, no podemos enrutar\n    if claim_info is None or severity_assessment is None:\n        return None\n\n    # Create input for the routing model - combina informaci√≥n de Stage 1 y Stage 2\n    routing_input = {\n        \"claim_id\": claim_info.claim_id,\n        \"name\": claim_info.name,\n        \"vehicle\": claim_info.vehicle,\n        \"loss_desc\": claim_info.loss_desc,\n        \"damage_area\": claim_info.damage_area,\n        \"severity\": severity_assessment.severity,\n        \"est_cost\": severity_assessment.est_cost\n    }\n\n    messages = [\n        {\"role\": \"system\", \"content\": queue_routing_system_prompt},\n        {\"role\": \"user\", \"content\": json.dumps(routing_input)},\n    ]\n\n    response = get_completion(messages=messages)\n\n    # Gate check: validate the routing decision\n    try:\n        validated_routing = gate3_validate_routing(response)\n        return validated_routing\n    except ValueError as e:\n        print(f\"Gate 3 failed: {e}. Response: {response}\")\n        return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the routing function on the sample data\n",
    "# No updates needed in this cell\n",
    "\n",
    "routed_claim_items = [\n",
    "    route_claim(claim, severity_assessment)\n",
    "    for claim, severity_assessment in zip(\n",
    "        extracted_claim_info_items, severity_assessment_items\n",
    "    )\n",
    "]\n",
    "\n",
    "routed_claim_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Outputs\n",
    "\n",
    "Let's put our data into a pandas dataframe for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# REVISI√ìN DE RESULTADOS: Crear DataFrame\n# ========================================\n# PROP√ìSITO: Combinar todos los resultados en un DataFrame de pandas para an√°lisis\n# CORREGIDO POR CLAUDE: Agregado manejo de None values\n\nimport pandas as pd\n\nrecords = []\nfor claim, severity_assessment, routed_claim in zip(\n    extracted_claim_info_items, severity_assessment_items, routed_claim_items\n):\n    record = {}\n    \n    # CORREGIDO: Solo agregar datos si no son None\n    # Si alg√∫n stage fall√≥, el resultado ser√° None y lo manejamos aqu√≠\n    if claim is not None:\n        record.update(claim.model_dump())  # Convertir Pydantic a dict\n    else:\n        record['claim_id'] = 'FAILED_EXTRACTION'\n        record['name'] = None\n        record['vehicle'] = None\n        record['loss_desc'] = None\n        record['damage_area'] = None\n    \n    if severity_assessment is not None:\n        record.update(severity_assessment.model_dump())  # Convertir Pydantic a dict\n    else:\n        record['severity'] = None\n        record['est_cost'] = None\n    \n    if routed_claim is not None:\n        record.update(routed_claim.model_dump())  # Convertir Pydantic a dict\n    else:\n        record['queue'] = None\n    \n    records.append(record)\n\n# Show the entire dataframe since it is not too large\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_colwidth\", None)\ndf = pd.DataFrame(records)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"RESULTADOS FINALES DEL PIPELINE DE CHAINING PROMPTS\")\nprint(\"=\"*80)\nprint(f\"\\nTotal de claims procesados: {len(df)}\")\nprint(f\"Claims exitosos (completaron todos los stages): {df['queue'].notna().sum()}\")\nprint(f\"Claims fallidos: {df['queue'].isna().sum()}\")\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\ndf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "üéâ Congratulations! üéâ You've built an impressive prompt chain system for insurance claims!\n",
    "You transformed messy FNOL text into structured data, assessed damage severity, and routed claims to the right queues, all with robust gate checks! üöÄ‚ú®\n",
    "\n",
    "Remember:\n",
    "\n",
    "- üîó Chained prompts break complex tasks into manageable steps\n",
    "- üõ°Ô∏è Gate checks prevent error cascades\n",
    "- üß† Having specialized prompts helps keep code focused and maintainable\n",
    "\n",
    "You've mastered a powerful pattern for countless business processes! üèÜ\n",
    "Amazing work on your agentic reasoning system! üíØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai_udacity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}